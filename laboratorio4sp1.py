# -*- coding: utf-8 -*-
"""Laboratorio4SP1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YSwv2VjjO7jBcRlkEsQTshscctEvZpm5

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Activation
from tensorflow.keras import backend as back

# Podria quitar os
import os
import numpy as np
import PIL.Image as Image
import matplotlib.pylab as plt

# Colocamos la seed para poder replicar los experimentos m√°s delante
seed=1998
np.random.seed(seed)
tf.random.set_seed(seed)

batch_size = 32
num_classes = 10
epochs = 20
data_augmentation = True
num_predictions = 20

"""# Dataset"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)



"""# Building the model"""

model = Sequential([
                    Conv2D(32, (3, 3), 
                           padding='same',
                           activation='relu',
                           input_shape=x_train.shape[1:]),
                    # Activation('relu'),
                    Conv2D(32, (3, 3), activation='relu'),
                    # Activation('relu'),
                    MaxPooling2D(pool_size=(2, 2)),
                    Dropout(0.25),
                    Conv2D(64, (3, 3), padding='same', activation='relu'),
                    # Activation('relu'),
                    Conv2D(64, (3, 3), activation='relu'),
                    # Activation('relu'),
                    MaxPooling2D(pool_size=(2, 2)),
                    Dropout(0.25),
                    Flatten(),
                    Dense(512, activation='relu'),
                    # Activation('relu'),
                    Dropout(0.5),
                    Dense(num_classes, activation='softmax')
                    # Activation('softmax')
])

opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)

# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

history = model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(x_test, y_test),
              shuffle=True)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10, 10))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1.0])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Cross Entropy')
plt.ylim([0.0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=3)

print('Loss:', test_loss)
print('Accuracy:', test_acc)

